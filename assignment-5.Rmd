---
title: "SAI-assignment-5"
author: "Daniel Verdejo 22240224"
date: "2022-12-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(dplyr)
library(tidyverse)
library(stats)
library(boot)
library(ggplot2)
library(GGally)
library(moderndive)
library(gridExtra)
library(leaps)
library(glmnet)
```

1. Question of Interest

2. Subjective Impressions or Exploratory Analysis

3. Formal Analysis

4. Conclusion and Translation


```{r, echo=FALSE}
clothing <- read.csv("./clothing.csv")

head(clothing)
names(clothing)
summary(clothing)

clothing <- clothing %>%
  filter(! Amount %in% c(0,1506000))
```

Lets first visualise the correlation and relationship between the explanatory variables and the target variable:

```{r echo=FALSE, message=FALSE, warning=FALSE}

# First lets look at the correlation to measure the linear relatioship between our variables then visualise some of these relationships

sp1 <- ggplot(clothing, aes(x = Recency, y = Amount)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Recency", y = "Amount", title = "Recency vs Amount")

sp2 <- ggplot(clothing, aes(x = Card, y = Amount)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Card", y = "Amount", title = "Card vs Amount")

sp3 <- ggplot(clothing, aes(x = Freq12, y = Amount)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Freq12", y = "Amount", title = "Freq12 vs Amount")

sp4 <- ggplot(clothing, aes(x = Dollar12, y = Amount)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Dollar12", y = "Amount", title = "Dollar12 vs Amount")

sp5 <- ggplot(clothing, aes(x = Freq24, y = Amount)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Freq24", y = "Amount", title = "Freq24 vs Amount")

sp6 <- ggplot(clothing, aes(x = Dollar24, y = Amount)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Dollar24", y = "Amount", title = "Dollar24 vs Amount")

grid.arrange(grobs= list(sp1,sp2,sp3,sp4,sp5,sp6), ncol=3, top="Scatter plots of explanatory variables vs Amount" )


clothing %>% ggpairs(clothing, progress = FALSE)
```
As we can see from the scatter plot of explanatory variables vs the target variable Amount there are linear relationships between the explanatory variables and the target variable.

Recency has a negative linear relationship while the Dollar12, and Dollar24 have positive linear relationships with the target variable, and the rest have weak relationships with the target variable. The Dollar12 and Dollar24 Have highly positive relationships while Amount vs Card, Freq12, and Freq24 all have weak relationships: In this case, the line suggests that the explanatory variables have little to no effect on the target variable. This is backed up by the weak correlation values we see between the Freq12 / Freq24 variables and Amount. They appear to have moderate correlation to Dollar12 / Dollar24: 
- Freq12 and Dollar12: 0.556
- Freq24 and Dollar24: 0.596

The Dollar 12, and Dollar24 both have strong correlations to the target variable Amount, with the Dollar12 being slightly stronger.
The Dollar12 and Dollar24 features also have a strong correlation between each other, which could indicate that they could both be measuring the same underlying concept.
The Freq12 and Freq24 also have a strong correlation between each other, and a weak correlation to the target variable, again indicating that they could both be measuring the same underlying concept.
The Recency is the only variable which has a negative correlation with all variables including the target variable.

We should first see what we can find out when we apply all explanatory variables against the multiple linear regresssion model (MLR), this should indicate where improvements can be made.

```{r}
model <- lm(Amount ~ Recency + Freq12 + Dollar12 + Freq24 + Dollar24 + Card, data = clothing)

summary(model)

regression_table <- get_regression_table(model = model)

regression_table
```
### Observations of the model

- Freq12 and Freq24 have extremely different coefficients, which given their strong positive association of 0.710 we may have expected them to be similar.
- Dollar12 and Dollar24 also have very differing coefficients, again given their strong positive association of 0.827 we also would have expected them to be similar.
- We can see that Dollar24, and Card have quite large p values despite them having moderate correlation values to the target variable this is somewhat contradictory of what we would expect.
- Furthermore, if we inspect the confidence intervals for these two in particular, we see that they both include zero.

If we look at the F-statistic we find that we have a large value for the F test statistic of 61.02 and a p value of 2.2e-16 which is lower than the 0.05 cutoff so its considered significant and we can reject the null hypothesis. All this is evidence to the explanatory variables and model being useful for explaining variation, so there is a contradiction from what discovered before.

The reason we are seeing these contradictions is multicollinearity. Multicollinearity is where two explanatory variables are measuring the same underlying concept or linearly related. Its Generally good practice to exclude one of the two explanatory variables that have a strong correlation between each other from the model to improve the accuracy of our predictions but more importantly reduce or eradicate the contradictions we have just discovered. first we will experiment and see what the 3 different scenarios produce (i.e how accurate is our prediction?).

Some findings to reinforce the decision to exclude expanatory variables are as follows:
- Given that Dollar12 is in the model the Dollar24 feature does not appear to add any more useful information for predicting the Amount, this is shown by Dollar24 having a high p-value of 0.956 and 95% confidence interval (-14.667, 43.916) which includes zero.
- We can also see that there is a strong correlation between Dollar12, and Dollar24 of ~ 0.827 from the previous correlation plot. Therefore if Dollar12 is included in the model, then Dollar24 is not needed (Dollar12 is preferable as it has a stronger correlation to the target variable than Dollar24)(add note about overfitting).
- We see a similar observation can be found for the Freq12 and Freq24 features, there is a strong correlation between the two features of ~ 0.710 again overfitting and same concept measured



```{r echo=FALSE}
row12 <- clothing[12, ]
print(row12)
```
lets do some predictions from our fitted model for row number 12 from our dataset:
The actual value is 50, the predicted amount if we calculate using the estimate from above:

On both plots we can see a positive linear correlation between the frequency and amount spent
```{r}
paste("Predicted value: ", regression_table$estimate[1] + prod(regression_table$estimate[2:7]),
    "  vs Actual value: ", row12$Amount)
```
The prediction is quite poor. This is likely due to some of the points we discussed earlier. 
We will carry out feature subset selection techniques in order to choose the best model.


First we will do the best subset regression
```{r}
best <- regsubsets(Amount ~ ., data = clothing)
summary(best)
```

```{r echo=FALSE}

best_adjr2_values <- summary(best)$adjr2
best_bic_values <- summary(best)$bic
best_r2_values <- summary(best)$rsq

# Create a data frame with the number of features and the adjr2 values
best_adjr2_df <- data.frame(Features = 1:length(best_adjr2_values), Value = best_adjr2_values)

# Create a data frame with the number of features and the bic values
best_bic_df <- data.frame(Features = 1:length(best_bic_values), Value = best_bic_values)

# Create a data frame with the number of features and the r2 values
best_r2_df <- data.frame(Features = 1:length(best_r2_values), Value = best_r2_values)

# Plot the adjr2 values
bp1 <-ggplot(best_adjr2_df, aes(x = Features, y = Value)) +
  geom_point(size=2, color = "coral") + geom_line(size=1, color = "coral") +
  labs(title = "Adjr2 Values", x = "Number of Features", y = "Value")

# Plot the bic values
bp2 <- ggplot(best_bic_df, aes(x = Features, y = Value)) +
  geom_point(size=2, color = "forestgreen") + geom_line(size=1, color = "forestgreen") +
  labs(title = "Bic Values", x = "Number of Features", y = "Value")

# Plot the r2 values
bp3 <- ggplot(best_r2_df, aes(x = Features, y = Value)) +
  geom_point(size=2, color = "dodgerblue") + geom_line(size=1, color = "dodgerblue") +
  labs(title = "R2 Values", x = "Number of Features", y = "Value")

grid.arrange(bp1, bp2, bp3, ncol=3, top = "Best subset regression")
```

```{r}

step(model, direction = "backward")   # backward selection

step(model, direction = "both")       # stepwise backwards


min.model <- lm(Amount ~ 1, data = clothing) # smallest model to consider

step(min.model, direction = "forward",   # forward selection
     scope = list(lower = ~ 1,
                  upper = ~ Recency + Freq12 + Dollar12 + Freq24 + Dollar24 + Card))

step(min.model, direction = "both",    # stepwise forward
     scope = list(lower = ~ 1,
                  upper = ~ Recency + Freq12 + Dollar12 + Freq24 + Dollar24 + Card))
```

```{r}
# Fit the model using glmnet() with the lasso method
lasso <- glmnet(x = as.matrix(clothing[,-1]), y = clothing$Amount, alpha = 1, lambda = 10^seq(-3, 6, 0.1))

plot(lasso, xvar = "lambda", label = TRUE)
```

From the above model selection process we can see a clear indication that the lowest AIC of **418.6** consistently uses the model `Amount ~ Dollar12 + Freq12` in all selection processes `backward, stepwise backward, forward, stepwise forward`. We also see similiar information from the best regression and exhaustive 

Parsimony
- It is desirable to find the simplest model required for the application, which is captured by the concept of parsimony:
"If two competing models have statistically the same predictive ability then the parsimonious model is the one with the smaller number of parameters
- Similar to the concept of Occam's Razor: "the simplest explanation is usually the right one"

Boot strapping 10000 samples

```{r}
#set the seed and then run the bootstrap samples
set.seed(22240224)
modelFn <- function(data, i) {
  m <- lm(Amount ~ Freq12 +
            Dollar12 +
            Freq12 * Dollar12 +
            Card, data = data[i, ])
  
  return(coef(m))
}

results <- boot(clothing, modelFn, R = 10000)

results

conf_intervals <- boot.ci(boot.out = results, type =  c("norm", "basic", "perc", "bca"))

conf_intervals
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
residuals <- residuals(model)

# Combine the residuals with the data
residuals_df <- cbind(clothing, residuals)

rp1 <- ggplot(residuals_df, aes(x = Recency, y = residuals)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Recency") +
  ylab("Residuals") +
  ggtitle("Residuals vs. Recency")
rp2 <- ggplot(residuals_df, aes(x = Freq12, y = residuals)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Freq12") +
  ylab("Residuals") +
  ggtitle("Residuals vs. Freq12")
rp3 <- ggplot(residuals_df, aes(x = Dollar12, y = residuals)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Dollar12") +
  ylab("Residuals") +
  ggtitle("Residuals vs. Dollar12")
rp4 <- ggplot(residuals_df, aes(x = Card, y = residuals)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Card") +
  ylab("Residuals") +
  ggtitle("Residuals vs. Card")

grid.arrange(grobs= list(rp1,rp2,rp3,rp4), ncols=2, top="Residuals vs explanatory variables" )
par(mfrow = c(2, 2))  # Set the layout of the plot to be 2x2
plot(model)
```



1. Using a subset selection procedure on the variables above, construct a model that best predict the Amount of money spent by a customer. Summarise the key features of the model fit and performance, and check the underlying assumptions.

When evaluating the fit and performance of a statistical model, there are
several key features that you should summarize and assess. These include:
1. The model's overall fit to the data, which can be assessed using measures
such as the R-squared value, the adjusted R-squared value, and the residual
standard error.
2. The statistical significance of the model's coefficients, which can be
assessed using hypothesis tests or confidence intervals.
3. The individual effects of the predictor variables on the response variable,
which can be assessed using measures such as t-tests, p-values, and
confidence intervals.
4. The model's predictive accuracy, which can be assessed using measures
such as the mean squared error (MSE), the root mean squared error (RMSE),
and the mean absolute error (MAE).

Additionally, when building and evaluating a statistical model, it is important to check the underlying assumptions of the model. These assumptions include:

1. Linearity: The relationship between the predictor and response variables is linear.
2. Normality: The residuals are normally distributed.
3. Independence: The observations are independent of each other.
4. Equal variances: The variances of the residuals are equal for all values of the predictor variable.
5. Outlier detection: The data does not contain any extreme values or outliers.


By summarizing the key features of the model fit and performance, and
checking the underlying assumptions, you can gain a better understanding of the model's strengths and weaknesses, and make informed decisions about how to improve it.

Assumptions Underlying the Model: LINE
1.Population relationship between the mean response and the
features is linear (Linearity assumption);
2.Sample is representative of the population and the subjects are
independent (Independence assumption);
3.Errors follow a normal distribution, centred about the regression
line (Normality assumption);
4.Variance of the errors is the same for any value of the explanatory
variable (Equal Spreads assumption).



